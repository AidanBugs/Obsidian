---
date: 01/30/25
tags:
  - Humanities
  - HW
links: 
deadline: 2025-02-09
status: 0
---
# 1
## (1) What does Bostrom mean by an “intelligence explosion”? Why does he believe that the upsides of superintelligence could be very good for humanity?

When Bostrom says intelligence explosion he means there will be a rapid development in AI research. This is because he believes that eventually AI research will be conducted by AI which will eventually be able to conduct research faster and faster as they essentially improve themselves. Bostrom argues that superintelligence will help develop better tools to deal with poverty, clean energy, medicine, etc. 

# 2
## (1) What are the three categories that superintelligent agents could fall into, according to Bostrom? List them and provide a short description of each.

1. Speed Superintelligence
- A system that can do anything intellectual task a human can do, but it can do it much faster.
2. Collective Superintelligence
- A system of smaller intellects that work together to achieve strong performance over a variety of domains. Similar to companies which are comprised of many humans working together across different domains to achieve good results.
3. Quality Superintelligence
- A system that is as fast as a human mind but is vastly smarter. The book compares how nonhuman animals are not as smart as humans (due to different factors such as complex structured languages or lack of planning ability), to the relationship of a quality superintelligence to humans.

## (2) What would each type of superintelligence excel in? (Bostrom discusses this at several points in the chapter, including in the “Direct and indirect reach” section.)

1. Speed Superintelligence
Speed superintelligence would excel at tasks that require the rapid execution of a long series of steps that must be performed sequentially.
2. Collective Superintelligence
Collective superintelligence would excel at tasks that can be parallelized into sub tasks or tasks that require a combination of different skill sets and/or perspectives.
3. Quality Superintelligence
Quality superintelligence would be able to solve problems that neither of the other two superintelligences would be able to solve. Bostrom also remarks that quality superintelligence is superior to human intelligence akin to how human intelligence is superior to the intelligence of a chimpanzee.  

## (3) Bostrom writes: “quality superintelligence would be the most capable form of all, inasmuch as it could grasp and solve problems that are, for all practical purposes, beyond the direct reach of speed superintelligence and collective superintelligence.” Why do you think he makes this claim?

He claims this because both speed and collective superintelligence are limited in the quality of their intellectual work. Bostrom gives the example where humans were unable to use complex linguistic representations, in which case humans might simply live in harmony with nature. This is important because in this sense both speed and collective superintelligence would also be unable to use complex linguistic representations. Quality superintelligence is superior because it is able to "think" beyond the scope of what is seen as possible to the human mind. 

# 3
## (1) What are the three considerations that could enable us to predict how a superintelligence might behave (that is, to predict its motivations)?

1. Predictability through design
2. Predictability through inheritance
3. Predictability through convergent instrumental reasons

## (2) What is the key idea encapsulated by the “orthogonality thesis”? Do you agree with it? Do you think that Bostrom is right that we shouldn’t “anthropomorphize” artificial intelligences, an idea that the orthogonality thesis partly aims to formalize?

The orthogonality thesis is about how any person with any level of intellgence can have any level of final goal. That is, a really smart person does not need to have a fancy goal in higher level academia, and instead could simply want to start a family. Also the opposite is true, a traditionally unintelligent person could have dreams of becomming a NASA engineer and building rockets for example. While I do agree with this thesis, there are certain caveats. One being that a unintelligent person will probably not be encouraged to go down the path of academia, instead their peers or teachers might discourage them throughout their life, thus they would be less likely to pursue a goal which requires a higher intellgence. For the extra smart person, their peers and teachers would encourage them to pursue higher level education and thus they could potentially gravitate toward that (and away from a "lower level" goal). However the success or realisticness of one's goal does not necessarily determine their goal. As such, it is possible for the people I listed above to have these opposing goals. 

As a computer scientist, I have heard many debates about wether or not we should humanize AI and if this is even the right goal with AI. In my opinion, in order to train AI we need data, and lots of it, as such these big AI companies scrape the web looking for whatever bits of interactions they may get. However, all the data we have to train AI is human data, human interactions with one another, human goals, etc. As such training an superintelligent AI that deviates from its training data is quite hard. As such, in terms of making a more "real" superintelligent AI, a human-like superintelligent AI might be easier than a non anthropomorphized AI.

## (3) What is the key idea expressed by the “instrumental convergence thesis,” and how does this thesis relate to the orthogonality thesis?

The key idea of the instrumental convergence thesis is that many goals have many overlapping values too them, thus even though people may pursue different goals they will share similar values. This relates to the orthogonality thesis because since people of varying intelligences can have the same and/or similar goals, then these people will also share similar values. As such, people with a wide range of intelligences will share values.

## (4) In your own words, what is the difference between “instrumental goals” and “final goals”?

Instrumental goals / values refer to intermediate objectives that must be obtained in order to reach one's final goal. Say for example we have Alice and Bob, Alice really wants to become an aerospace engineer while bob wants become a software engineer. Alice and Bob clearly have different final goals, however an instrumental goal they both have could be to graduate from college.

## (5) Why does Bostrom think that “self-preservation” is an intermediate—or instrumental—goal that any sufficiently intelligent agent will converge upon?

Simply put it, in order for a person to achieve their final goal, they usually need to be alive. As such, self-preservation as an instrumental / intermediate goal is important because it helps ensure their survival in order to reach their final goal

## (6) What is the argument for why a superintelligent agent would want to prevent its final goals from being changed? (See the “Goal-content integrity” section.)

For starters, by maintaining the same goal over a long period of time they are more likely to achieve it than by constantly switching goals. In the case of superintelligent agents whom can swap bodies, download skills etc, goal preservation is a must. This is because if goals are not preserved, then as these agents download more and skills, eventually this would create a homogenous society where each superintelligent agent is more or less the same. In such a society the author refers to them as a "functional soup". If goals are not preserved, then our soup will all be pursing the same goal as opposed to a diverse set of goals which are necessary for the society to function. 

## (7) Briefly, why would a superintelligent agent strive to enhance its cognition, perfect technology, and acquire resources?

Improving cognition improves one's decision making, leading the superintelligent agent to more likely reach their final goal. Technological improvements lead to an increased effeciency in their "mental performance" and can also increase their "physical" output. Thus a superintelligent agent would certainly wish to increase their technology regardless if their final goal is more cognitive or material. Acquisition of resources is also important for more material goals because with more resources the more materialistic things can be built, constructed, maintained etc. 

## (8) Do you find Bostrom’s argument compelling? Do you think that a superintelligent agent could be motivated by a wide range of final goals, some of which might seem absurd to us simply because we have evolved over millions of years to think that certain goals “make sense” while other goals don’t (orthogonality thesis)? Do you think that, no matter what the final goals that a superintelligence might have, it will—simply by virtue of being intelligent—converge upon a finite set of intermediate/instrumental goals like self-preservation and resource acquisition (instrumental convergence thesis)?

I really enjoyed reading about Bostrom's arguement with superintelligent goals and intermediate goals. I do believe that superintelligent agents will be motivated by a wide variety of goals. However, for the goals that we deem "absurd" will stem from the fact that us as humans believed them to be impossible because we did not fully unlock the intermediate goals. However, once these goals have been brought to light by the superintelligent agents, humans will help these superintelligent agents pursue their goal (and thus some humans will change their goals) due to the impact it will leave on society. For example, humans believed flying was impossible because we didn't fully understand the subgoal of aerodynamics. But once the wright brothers experimented with aerodynamics, we were able to make a plane which flew in the air. The same could potentially be said with teleportation (seems absurd but maybe its possible with superintelligence). If a superintelligent agent fully learns the physics of the universe then teleportation might be more realistic than we currently believe. Thus, us humans will help superintelligent agents in their goals despite us being the intelectually inferior beings (orthogonality thesis). As stated above, I believe that superintelligent agents will help us pursue more advanced final goals by changing the scope at which we view instrumental goals. As such, I believe the opposite is true when it comes to superintelligent beings converging into a set of finite goals.  

## (9) Why does Bostrom think that a superintelligence would spread into the universe?

Bostrom believes that superintelligent agents would find a very wide variety of uses for many resources. Examples being increasing technology, creating defense systems, create additional physical or simulated lives, etc. Bostrom argues that these wide variety of resource use will easily consume an entire planet's worth of resources. As such, superintelligent agents must pursue extraterrestrial endeavors in order to counteract the depletion of resources.

# 4
## (1) In brief, what is the argument that Bostrom presents at the beginning of this chapter for why the “default outcome” of superintelligent might be “doom”—i.e., the annihilation of everyone on the planet? (Hint: it consists of three premises, two of which were discussed in the previous chapter.)

Firstly, the superintelligence would obtain a strategic advantage and would be able to dictate the future of human life on earth. Second, due to the orthogonality thesis superintelligent agents may not share any values associated with higher intellect. Thus, we are unable to predict their final goal which would be their driving motivation. Third, the instrumental convergence thesis suggests that even if the superintelligence has a final goal of "finding pi", it would not limit its intermediate goals to reach its final goal. As an extreme example, if the superintelligence needs more resources, there would not be anything stopping it from completely depleting earth of all of its resources. 

## (2) In your own words, what is “the treacherous turn”?

The treacherous turn is the idea that an intelligent malicious AI would be aware it is in a test environment and needs to pass the positive behaviour checks. As such, while in the "sandbox" the AI will behave accordingly, until it obtains enough freedom (resources, WWW access, etc) to unleash its full power.

## (3) What is the point of Bostrom’s examples of “perverse instantiation”? What is the danger that Bostrom is trying to highlight?

The point of Bostrom's examples is that even if the programmers have good intentions with the superintelligence, the superintelligence might act in a way that fulfills the programmer's request but not how they intended. The danger is that the superintelligence would act extremely consequentialist and only focus on the outcome without any regard to potential dangers or sacrifices made along the way. 

## (4) What is “wireheading,” and how might a superintelligent agent be susceptible to it? (This is a hard question, so just do your best!)

The idea with wireheading is when training a superintelligence a programmer would associate a reward with an action in order to train an appropriate behaviour, then the superintelligence would abuse the reward system. The example provided is the programmer suggest the superintelligence to minimize the time until the next reward and the superintelligence accomplishes this by short circuiting the reward system so they always recieve a reward. The superintelligent agent is susceptible to this because it is simply trying to obtain the rewards as much and as fast as possible and wireheading is certainly one way for the agent to do it (although certainly not what the programmer wants). 

## (5) What does Bostrom mean by “infrastructure profusion”? What is the connection between infrastructure profusion and wireheading? Give another example—one that doesn’t involve wireheading—of how infrastructure profusion could have catastrophic consequences for humanity.

 

## (6) Many leading figures in the field of AI today have read Bostrom’s Superintelligence, and many agree that superintelligence carries with it the risk of total human annihilation. For example, OpenAI is explicitly trying to build superintelligent machines (a type of “artificial general intelligence,” or AGI—a more common term these days). This company was co-founded by Sam Altman, who wrote in 2015 that the “development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.” In 2023, he said that “the bad case” with respect to AGI and superintelligence is “lights out for all of us.” At another point, he declared that “I think that AI will … most likely sort of lead to the end of the world, but in the meantime there will be great companies created with serious machine learning,” and “probably AI will kill us all, but until then we’re going to turn out a lot of great students.” Do you agree with Altman and Bostrom that superintelligence (or AGI) could pose an “existential risk” to humanity? Do you think it’s wise for companies like OpenAI to be racing to build superintelligent machines? Or do you think the risks aren’t worth the potential benefits (cures for diseases, huge profits for companies like Microsoft, and so on)?



# 5
## (1) Do you agree with Leopold Aschenbrenner’s claim that we need a “Manhattan Project” for AGI/superintelligence? Why do you think that Aschenbrenner worries about efforts outside of the US to build AGI/superintelligence posing a national security risk?



